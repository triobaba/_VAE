{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2945\n",
      "Epoch [2/100], Loss: 0.0992\n",
      "Epoch [3/100], Loss: 0.0589\n",
      "Epoch [4/100], Loss: 0.0518\n",
      "Epoch [5/100], Loss: 0.0495\n",
      "Epoch [6/100], Loss: 0.0484\n",
      "Epoch [7/100], Loss: 0.0478\n",
      "Epoch [8/100], Loss: 0.0476\n",
      "Epoch [9/100], Loss: 0.0475\n",
      "Epoch [10/100], Loss: 0.0472\n",
      "Epoch [11/100], Loss: 0.0467\n",
      "Epoch [12/100], Loss: 0.0468\n",
      "Epoch [13/100], Loss: 0.0467\n",
      "Epoch [14/100], Loss: 0.0465\n",
      "Epoch [15/100], Loss: 0.0464\n",
      "Epoch [16/100], Loss: 0.0462\n",
      "Epoch [17/100], Loss: 0.0463\n",
      "Epoch [18/100], Loss: 0.0466\n",
      "Epoch [19/100], Loss: 0.0462\n",
      "Epoch [20/100], Loss: 0.0463\n",
      "Epoch [21/100], Loss: 0.0462\n",
      "Epoch [22/100], Loss: 0.0462\n",
      "Epoch [23/100], Loss: 0.0462\n",
      "Epoch [24/100], Loss: 0.0460\n",
      "Epoch [25/100], Loss: 0.0460\n",
      "Epoch [26/100], Loss: 0.0459\n",
      "Epoch [27/100], Loss: 0.0459\n",
      "Epoch [28/100], Loss: 0.0461\n",
      "Epoch [29/100], Loss: 0.0460\n",
      "Epoch [30/100], Loss: 0.0459\n",
      "Epoch [31/100], Loss: 0.0460\n",
      "Epoch [32/100], Loss: 0.0457\n",
      "Epoch [33/100], Loss: 0.0459\n",
      "Epoch [34/100], Loss: 0.0458\n",
      "Epoch [35/100], Loss: 0.0458\n",
      "Epoch [36/100], Loss: 0.0457\n",
      "Epoch [37/100], Loss: 0.0458\n",
      "Epoch [38/100], Loss: 0.0457\n",
      "Epoch [39/100], Loss: 0.0457\n",
      "Epoch [40/100], Loss: 0.0457\n",
      "Epoch [41/100], Loss: 0.0457\n",
      "Epoch [42/100], Loss: 0.0457\n",
      "Epoch [43/100], Loss: 0.0457\n",
      "Epoch [44/100], Loss: 0.0456\n",
      "Epoch [45/100], Loss: 0.0458\n",
      "Epoch [46/100], Loss: 0.0457\n",
      "Epoch [47/100], Loss: 0.0457\n",
      "Epoch [48/100], Loss: 0.0457\n",
      "Epoch [49/100], Loss: 0.0456\n",
      "Epoch [50/100], Loss: 0.0457\n",
      "Epoch [51/100], Loss: 0.0456\n",
      "Epoch [52/100], Loss: 0.0458\n",
      "Epoch [53/100], Loss: 0.0457\n",
      "Epoch [54/100], Loss: 0.0457\n",
      "Epoch [55/100], Loss: 0.0457\n",
      "Epoch [56/100], Loss: 0.0457\n",
      "Epoch [57/100], Loss: 0.0457\n",
      "Epoch [58/100], Loss: 0.0457\n",
      "Epoch [59/100], Loss: 0.0456\n",
      "Epoch [60/100], Loss: 0.0456\n",
      "Epoch [61/100], Loss: 0.0455\n",
      "Epoch [62/100], Loss: 0.0457\n",
      "Epoch [63/100], Loss: 0.0456\n",
      "Epoch [64/100], Loss: 0.0457\n",
      "Epoch [65/100], Loss: 0.0456\n",
      "Epoch [66/100], Loss: 0.0457\n",
      "Epoch [67/100], Loss: 0.0456\n",
      "Epoch [68/100], Loss: 0.0459\n",
      "Epoch [69/100], Loss: 0.0456\n",
      "Epoch [70/100], Loss: 0.0457\n",
      "Epoch [71/100], Loss: 0.0456\n",
      "Epoch [72/100], Loss: 0.0456\n",
      "Epoch [73/100], Loss: 0.0457\n",
      "Epoch [74/100], Loss: 0.0457\n",
      "Epoch [75/100], Loss: 0.0457\n",
      "Epoch [76/100], Loss: 0.0455\n",
      "Epoch [77/100], Loss: 0.0456\n",
      "Epoch [78/100], Loss: 0.0456\n",
      "Epoch [79/100], Loss: 0.0457\n",
      "Epoch [80/100], Loss: 0.0456\n",
      "Epoch [81/100], Loss: 0.0458\n",
      "Epoch [82/100], Loss: 0.0456\n",
      "Epoch [83/100], Loss: 0.0456\n",
      "Epoch [84/100], Loss: 0.0457\n",
      "Epoch [85/100], Loss: 0.0455\n",
      "Epoch [86/100], Loss: 0.0457\n",
      "Epoch [87/100], Loss: 0.0456\n",
      "Epoch [88/100], Loss: 0.0457\n",
      "Epoch [89/100], Loss: 0.0456\n",
      "Epoch [90/100], Loss: 0.0456\n",
      "Epoch [91/100], Loss: 0.0454\n",
      "Epoch [92/100], Loss: 0.0456\n",
      "Epoch [93/100], Loss: 0.0456\n",
      "Epoch [94/100], Loss: 0.0457\n",
      "Epoch [95/100], Loss: 0.0457\n",
      "Epoch [96/100], Loss: 0.0457\n",
      "Epoch [97/100], Loss: 0.0456\n",
      "Epoch [98/100], Loss: 0.0456\n",
      "Epoch [99/100], Loss: 0.0456\n",
      "Epoch [100/100], Loss: 0.0455\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = pd.read_csv(data_file)\n",
    "        # Normalize input features between 0 and 1\n",
    "        self.X = self.data[['X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', \n",
    "                            'X_7', 'X_8', 'X_9', 'X_10', 'X_11', 'X_12']].values\n",
    "        self.X = self.X / np.max(self.X, axis=0)  # Normalize per feature\n",
    "        self.y = self.data['y'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Encoder: maps input to latent space\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_size * 2)  # Outputs mu and logvar\n",
    "        )\n",
    "\n",
    "        # Decoder: reconstructs input from latent space\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_size),\n",
    "            nn.Sigmoid()  # Ensures output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = encoded[:, :self.latent_size], encoded[:, self.latent_size:]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset('assignment_dataset.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "vae = VAE(input_size=12, latent_size=8)  # Increased latent size\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-4)  # Reduced learning rate\n",
    "criterion = nn.MSELoss()\n",
    "beta = 0.1  # Balancing factor for KL divergence\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for x, _ in dataloader:\n",
    "        x = x.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon, mu, logvar = vae(x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        recon_loss = criterion(recon, x)\n",
    "        kl_divergence = 0.5 * torch.sum(mu ** 2 + torch.exp(logvar) - logvar - 1)\n",
    "        loss = criterion(recon, x) + beta * 0.5 * torch.sum(mu ** 2 + torch.exp(logvar) - logvar - 1)\n",
    "\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'vae_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(vae.state_dict(), 'vae_model.pth')\n",
    "print(\"Model saved as 'vae_model.pth'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "vae.eval()  # Set the model to evaluation mode\n",
    "latent_samples = torch.randn(1000, vae.latent_size)  # Sample from latent space\n",
    "synthetic_data = vae.decoder(latent_samples).detach().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data saved as 'synthetic_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save synthetic data to CSV\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[f'X_{i+1}' for i in range(12)])\n",
    "synthetic_df.to_csv('synthetic_data.csv', index=False)\n",
    "print(\"Synthetic data saved as 'synthetic_data.csv'\")\n",
    "\n",
    "# Load model and generate new data (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=12, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vae = VAE(input_size=12, latent_size=8)\n",
    "loaded_vae.load_state_dict(torch.load('vae_model.pth'))\n",
    "loaded_vae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New synthetic data generated!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latent_samples = torch.randn(1000, loaded_vae.latent_size)\n",
    "new_synthetic_data = loaded_vae.decoder(latent_samples).detach().numpy()\n",
    "print(\"New synthetic data generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Statistics:\n",
      "                X_0           X_1           X_2           X_3           X_4  \\\n",
      "count  10000.000000  10000.000000  10000.000000  1.000000e+04  10000.000000   \n",
      "mean      -0.043590      0.105652      2.149697  2.672754e+00      2.733670   \n",
      "std        2.749667      1.661659      1.714908  3.965479e+00      0.195528   \n",
      "min      -13.097479     -7.456419      0.000534  3.794138e-09      0.000000   \n",
      "25%       -1.833648     -0.960446      0.831932  2.738268e-01      2.628441   \n",
      "50%       -0.112898      0.135436      1.766501  1.168059e+00      2.750429   \n",
      "75%        1.697181      1.215033      3.061595  3.445960e+00      2.857354   \n",
      "max       11.517463      7.916655     13.097479  4.584538e+01      3.376908   \n",
      "\n",
      "                X_5           X_6           X_7           X_8           X_9  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       0.397514     -0.389984     -0.041177     -0.184348     -0.027133   \n",
      "std        2.429113      1.774155      1.616021      2.465357      1.539639   \n",
      "min      -10.900439     -6.808179     -5.870001    -10.099261     -6.463066   \n",
      "25%       -1.073380     -1.587190     -1.134441     -1.748086     -1.019801   \n",
      "50%        0.464597     -0.440098     -0.059403     -0.181802     -0.037908   \n",
      "75%        1.929343      0.793696      1.019622      1.341018      0.964056   \n",
      "max       11.667105      6.464188      6.906435     10.505540      5.738035   \n",
      "\n",
      "               X_10          X_11          X_12             y  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  \n",
      "mean      -0.089901     -0.184348      0.005321      4.492485  \n",
      "std        1.671733      2.465357      1.599455      2.909414  \n",
      "min       -7.078541    -10.099261     -6.403227     -1.750572  \n",
      "25%       -1.162538     -1.748086     -1.013607      2.017560  \n",
      "50%       -0.041843     -0.181802     -0.031708      4.489248  \n",
      "75%        1.008065      1.341018      1.022664      6.995868  \n",
      "max        6.854905     10.505540      6.795592     10.945960  \n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv('assignment_dataset.csv')\n",
    "synthetic_data = pd.read_csv('synthetic_data.csv')\n",
    "\n",
    "print(\"Original Data Statistics:\")\n",
    "print(original_data.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic Data Statistics:\n",
      "               X_1          X_2          X_3          X_4          X_5  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean      0.009720     0.164849     0.058832     0.806773     0.033667   \n",
      "std       0.005879     0.007535     0.005662     0.007079     0.007488   \n",
      "min       0.000578     0.136883     0.041293     0.777251     0.012911   \n",
      "25%       0.005166     0.160347     0.055297     0.802205     0.028669   \n",
      "50%       0.008488     0.164869     0.058928     0.806869     0.034119   \n",
      "75%       0.013079     0.169411     0.062441     0.811110     0.039113   \n",
      "max       0.032441     0.197905     0.085190     0.830168     0.051226   \n",
      "\n",
      "                X_6           X_7           X_8           X_9          X_10  \\\n",
      "count  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03   \n",
      "mean   3.394686e-05  1.786560e-04  8.425759e-05  2.348946e-04  1.153173e-04   \n",
      "std    8.046373e-05  3.225415e-04  1.656361e-04  4.158338e-04  2.164215e-04   \n",
      "min    2.034876e-10  2.510645e-08  6.165369e-09  3.425406e-08  9.194507e-09   \n",
      "25%    9.407666e-07  1.333291e-05  5.108568e-06  1.854724e-05  8.000054e-06   \n",
      "50%    5.548014e-06  5.443581e-05  2.176406e-05  7.340553e-05  3.281625e-05   \n",
      "75%    2.729603e-05  1.894171e-04  8.317927e-05  2.580155e-04  1.196595e-04   \n",
      "max    7.624933e-04  2.638899e-03  1.489039e-03  3.375428e-03  1.750853e-03   \n",
      "\n",
      "               X_11          X_12  \n",
      "count  1.000000e+03  1.000000e+03  \n",
      "mean   8.961370e-05  5.959943e-04  \n",
      "std    1.831316e-04  8.630801e-04  \n",
      "min    3.126257e-09  4.068236e-07  \n",
      "25%    4.408843e-06  8.104285e-05  \n",
      "50%    2.087724e-05  2.563296e-04  \n",
      "75%    8.577004e-05  7.250545e-04  \n",
      "max    1.611452e-03  6.165061e-03  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSynthetic Data Statistics:\")\n",
    "print(synthetic_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SYNTHENSIS ASSESSMENT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity Assessment using Kolmogorov-Smirnov Test:\n",
      "X_1: KS Statistic = 0.5272, P-Value = 0.0000\n",
      "X_2: KS Statistic = 0.9383, P-Value = 0.0000\n",
      "X_3: KS Statistic = 0.8616, P-Value = 0.0000\n",
      "X_4: KS Statistic = 0.9999, P-Value = 0.0000\n",
      "X_5: KS Statistic = 0.5757, P-Value = 0.0000\n",
      "X_6: KS Statistic = 0.5943, P-Value = 0.0000\n",
      "X_7: KS Statistic = 0.5139, P-Value = 0.0000\n",
      "X_8: KS Statistic = 0.5337, P-Value = 0.0000\n",
      "X_9: KS Statistic = 0.5093, P-Value = 0.0000\n",
      "X_10: KS Statistic = 0.5123, P-Value = 0.0000\n",
      "X_11: KS Statistic = 0.5337, P-Value = 0.0000\n",
      "X_12: KS Statistic = 0.5082, P-Value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Load original dataset\n",
    "original_data = pd.read_csv('assignment_dataset.csv')\n",
    "\n",
    "# Load synthetic dataset\n",
    "synthetic_data = pd.read_csv('synthetic_data.csv')\n",
    "\n",
    "# Select only the X columns for comparison\n",
    "original_X = original_data[[f'X_{i+1}' for i in range(12)]]\n",
    "synthetic_X = synthetic_data[[f'X_{i+1}' for i in range(12)]]\n",
    "\n",
    "# Function to compute KS-test for each feature\n",
    "def assess_fidelity_ks(original, synthetic):\n",
    "    results = {}\n",
    "    for column in original.columns:\n",
    "        ks_stat, p_value = ks_2samp(original[column], synthetic[column])\n",
    "        results[column] = {'KS Statistic': ks_stat, 'P-Value': p_value}\n",
    "    return results\n",
    "\n",
    "# Run KS-test on all features\n",
    "fidelity_results = assess_fidelity_ks(original_X, synthetic_X)\n",
    "\n",
    "# Print the results\n",
    "print(\"Fidelity Assessment using Kolmogorov-Smirnov Test:\")\n",
    "for feature, result in fidelity_results.items():\n",
    "    print(f\"{feature}: KS Statistic = {result['KS Statistic']:.4f}, P-Value = {result['P-Value']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
